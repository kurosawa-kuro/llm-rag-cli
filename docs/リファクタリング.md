非常に整理された設計です。
この構成で「どこを攻めると精度が伸びるか」を、影響度 × 改修コスト × CPU前提 で整理します。

結論：精度のボトルネックは3層

LLM精度はこの3層で決まります。

① チャンク設計   ← 取り込み品質
② 検索品質       ← Recall/順位
③ 生成制御       ← 幻覚抑制・情報圧縮

あなたの現在構成だと、最優先は①と②です。

理由：

Llama-2-7B Q4 は生成能力は限定的

だから「渡す文脈の質」がほぼ全てを決める

① 最重要：チャンク最適化（ここが一番伸び代大）

今：

chunk_size=500

overlap=100

PDFは段落分割

CSVは固定分割

問題になりやすい点

500文字はやや長い（埋め込みがぼやける）

段落＝意味単位とは限らない

CSVが意味単位になっていない可能性

攻め方（具体策）
✅ ① chunk_size を 300〜400 に落とす

理由：

MiniLM（384次元）は長文に弱い

短いほうがベクトルがシャープになる

まずは：

CHUNK_SIZE=350
OVERLAP=80

で再評価。

✅ ② セマンティックチャンク導入（可能なら）

今は構造ベース。
理想は：

文単位で分割

embedding距離で意味の切れ目を検出

類似度が急落する箇所で区切る

これやると Retrieval@k が大きく伸びます。

CPUでも可能です。

✅ ③ CSVは「1行=1ドキュメント」に固定

RecursiveSplitterは不要です。

FAQや商品情報は：

1行 → 1 embedding

のほうが精度出ます。

② Retrieval層（2番目に効く）

あなたはTwoStageやっているので優秀です。

ただし、今の設定：

SEARCH_K = 10
RERANK_TOP_K = 3

これはやや弱い。

攻め方
✅ ① SEARCH_K を 20 に増やす

CrossEncoderは強い。
候補を増やすと精度上がります。

SEARCH_K = 20
RERANK_TOP_K = 5

CPUでも十分回ります。

✅ ② MMR導入（重要）

pgvectorは純cosine。

以下を試すと伸びます：

search_type="mmr"
lambda_mult=0.5

冗長チャンク排除できます。

✅ ③ CrossEncoder変更

今：

ms-marco-MiniLM-L-6-v2

これは英語最適。

日本語文書なら：

cross-encoder/ms-marco-MiniLM-L-12-v2

か、
日本語対応モデルに変えると伸びます。

③ 生成層（LLM制御）

ここは「幻覚抑制」が主戦場。

今のプロンプト：

以下の情報を基に回答してください:
...

これは弱い。

攻め方
✅ ① 強制抽出型プロンプトに変更
以下のコンテキスト以外の知識を使用してはいけません。
情報が不足している場合は「情報が見つかりません」と答えてください。

これだけで Faithfulness 上がります。

✅ ② 回答フォーマットを固定
【回答】
...

【根拠】
- source1
- source2

形式固定で hallucination 減ります。

④ Embedding層（中〜大改善余地）

今：

all-MiniLM-L6-v2

これは軽量だが強くない。

攻め方（CPU前提）
候補①（最有力）
intfloat/multilingual-e5-base

精度かなり上がります。
CPU可。

候補②（最強だが重い）
bge-m3

CPUだと遅いが精度最強クラス。

優先順位（あなた向け）

あなたは設計が整っているので、順番はこうです。

🥇 1位：チャンク最適化

size=350

CSV 1行1doc

セマンティック分割

🥈 2位：SEARCH_K拡張 + MMR
🥉 3位：EmbeddingをE5へ変更
4位：プロンプト強化
5位：LLM変更（最後）

LLMを変えるより、
Retrievalを強くするほうが10倍効きます。

数値で見る改善インパクト
改善	伸び幅
chunk最適化	★★★★☆
SEARCH_K拡張	★★★☆☆
Embedding変更	★★★★☆
Prompt改善	★★☆☆☆
LLM変更	★☆☆☆☆
本質

RAGの精度は：

LLM性能  <  Retrieval精度

です。

今の構成なら、
LLMを触る前に Retrievalを極限まで鍛えるのが正解です。

もしよろしければ、

現在の evaluate レポート結果（Retrieval@k / Faithfulness）を教えてください。

そこから、どこがボトルネックかをロジカルに切り分けます。

精度向上を記録するライブラリとかコード構成とかありませんか？

非常に良い視点です。
「改善したつもり」ではなく 定量的に伸びを記録する仕組み を入れるのは、RAGでは必須です。

結論

あなたの構成なら、

✅ 外部SaaS不要
✅ CPU完結
✅ 既存 evaluate.py を拡張

で十分戦えます。

選択肢は3レベルあります
🥇 レベル1（最も現実的）：自前実験トラッカーを作る

あなたの設計はすでに評価基盤がある。

rag.evaluation.evaluate
rag.evaluation.metrics

ここに 実験ログ保存機能を足すだけ で完成します。

やること
① 実験IDを発行

例：

from datetime import datetime

experiment_id = datetime.now().strftime("%Y%m%d_%H%M%S")
② JSONで保存

例：

result = {
    "experiment_id": experiment_id,
    "chunk_size": CHUNK_SIZE,
    "overlap": CHUNK_OVERLAP,
    "search_k": SEARCH_K,
    "rerank_top_k": RERANK_TOP_K,
    "embed_model": EMBED_MODEL,
    "llm": LLM_MODEL_PATH,
    "retrieval_at_k": retrieval_score,
    "faithfulness": faithfulness_score,
    "exact_match": exact_score,
    "latency": avg_latency,
}

with open("experiments/history.jsonl", "a") as f:
    f.write(json.dumps(result) + "\n")
なぜこれが強いか

あなたの構成は：

DI Container

config管理

evaluateパイプラインあり

なので、実験管理は あと1ファイル追加するだけで完成 します。

🥈 レベル2：MLflow（本格運用向け）
🔹 MLflow

CPUだけで動く実験管理ツール。

特徴：

パラメータ記録

メトリクス記録

実験比較UI

Docker対応

ただし：

少し重い

過剰設計になりやすい

PoC段階ならまだ不要です。

🥉 レベル3：RAG専用評価フレームワーク
1️⃣ Ragas

Faithfulness

Context Precision

Context Recall

Answer Relevancy

をLLMで自動評価。

ただし：

LLM API必要（外部API依存）

ローカルだけだと評価が不安定

あなたの「CPU完結思想」とは少し相性が悪い。

2️⃣ TruLens

RAGの可視化特化。

Retrieval品質分析

Attribution確認

しかし導入は重め。

🔥 あなたに最適な構成

あなたは：

CLI型

Docker化済み

make evaluateあり

PoC設計志向

なので、

🔥 ベストは「実験管理モジュールを自作」

構成例：

rag/
  evaluation/
    experiment.py  ← 追加
experiment.py の役割
- 実験ID生成
- 設定スナップショット保存
- 評価結果保存
- 比較用ロード機能
さらに一段上の設計（あなた向け）
「改善差分」を自動算出する

例：

前回との差分:
Retrieval@k: +8.2%
Faithfulness: +3.1%
Latency: -0.4s

これを自動表示するだけで、

あなたのRAGは「研究可能なシステム」になります。

🧠 精度改善を本当にやる人の構成

最終形はこうなります：

make evaluate
↓
history.jsonl に保存
↓
make compare
↓
差分表示
🔥 本質

RAG精度向上は：

改善 → 記録 → 比較 → 仮説修正

このループが全てです。

LLMを変えるより、
このループを作るほうが10倍重要です。

もしよろしければ：

次の段階として

実験トラッカー設計書

experiment.py の具体実装コード

compareコマンド追加案

ディレクトリ設計変更案

どこまで踏み込みますか？

設計レベルで整理しますか？
実装コードまで出しますか？

重要箇所のみ抜粋依頼

実験トラッカー設計書

experiment.py の具体実装コード

compareコマンド追加案

ディレクトリ設計変更案

承知しました。
重要箇所のみに絞って、すぐ実装できる形で提示します。

① 実験トラッカー設計書（抜粋・最小構成）
🎯 目的

精度改善を 再現可能な形で記録

パラメータと評価結果を紐付け

差分比較を可能にする

🧱 設計方針
1. JSONLで追記保存（1行=1実験）

理由：

Git管理しやすい

追記のみで壊れにくい

後からPandasで解析可能

📦 保存フォーマット（最重要）
{
  "experiment_id": "20260221_153210",
  "timestamp": "2026-02-21T15:32:10",
  "config": {
    "chunk_size": 350,
    "chunk_overlap": 80,
    "search_k": 20,
    "rerank_top_k": 5,
    "embed_model": "intfloat/multilingual-e5-base",
    "llm": "llama-2-7b.Q4_K_M.gguf"
  },
  "metrics": {
    "retrieval_at_k": 0.92,
    "faithfulness": 0.87,
    "exact_match": 0.76,
    "latency": 1.82
  }
}
🔁 改善ループ設計
make evaluate
↓
experiment.save()
↓
history.jsonl
↓
make compare
↓
差分表示
② experiment.py（具体実装コード）

src/rag/evaluation/experiment.py

import json
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, List

HISTORY_PATH = Path("experiments/history.jsonl")


def _ensure_dir() -> None:
    HISTORY_PATH.parent.mkdir(parents=True, exist_ok=True)


def generate_experiment_id() -> str:
    return datetime.now().strftime("%Y%m%d_%H%M%S")


def save_experiment(config: Dict[str, Any], metrics: Dict[str, Any]) -> str:
    _ensure_dir()

    experiment_id = generate_experiment_id()

    record = {
        "experiment_id": experiment_id,
        "timestamp": datetime.now().isoformat(),
        "config": config,
        "metrics": metrics,
    }

    with open(HISTORY_PATH, "a", encoding="utf-8") as f:
        f.write(json.dumps(record, ensure_ascii=False) + "\n")

    return experiment_id


def load_history() -> List[Dict[str, Any]]:
    if not HISTORY_PATH.exists():
        return []

    with open(HISTORY_PATH, "r", encoding="utf-8") as f:
        return [json.loads(line) for line in f if line.strip()]


def get_last_n(n: int = 2) -> List[Dict[str, Any]]:
    history = load_history()
    return history[-n:]
③ evaluate.py への追加（重要部分のみ）

評価完了後に追記：

from rag.evaluation.experiment import save_experiment
from rag.core.config import (
    CHUNK_SIZE,
    CHUNK_OVERLAP,
    SEARCH_K,
    RERANK_TOP_K,
    EMBED_MODEL,
    LLM_MODEL_PATH,
)

config_snapshot = {
    "chunk_size": CHUNK_SIZE,
    "chunk_overlap": CHUNK_OVERLAP,
    "search_k": SEARCH_K,
    "rerank_top_k": RERANK_TOP_K,
    "embed_model": EMBED_MODEL,
    "llm": LLM_MODEL_PATH,
}

metrics_snapshot = {
    "retrieval_at_k": retrieval_score,
    "faithfulness": faithfulness_score,
    "exact_match": exact_score,
    "latency": avg_latency,
}

save_experiment(config_snapshot, metrics_snapshot)

これで毎回履歴が残ります。

④ compare コマンド追加案
compare.py 新規作成

src/rag/evaluation/compare.py

from rag.evaluation.experiment import get_last_n


def compare_last_two() -> None:
    experiments = get_last_n(2)

    if len(experiments) < 2:
        print("比較対象が不足しています。")
        return

    prev, curr = experiments

    print("=== Compare Result ===\n")

    for key in curr["metrics"].keys():
        prev_val = prev["metrics"][key]
        curr_val = curr["metrics"][key]
        diff = curr_val - prev_val

        sign = "+" if diff >= 0 else ""
        print(f"{key}: {curr_val:.3f} ({sign}{diff:.3f})")
Makefile 追加
compare:
	python -m rag.evaluation.compare
⑤ ディレクトリ設計変更案（最小追加）
llm-rag-cli/
├── experiments/               ← NEW
│   └── history.jsonl
│
├── src/
│   └── rag/
│       └── evaluation/
│           ├── experiment.py  ← NEW
│           └── compare.py     ← NEW
🔥 さらに一段上の拡張（将来）

configハッシュを保存（同一設定検出）

ベストスコア抽出関数

CSV出力

Matplotlibで推移可視化

本質

あなたのRAGはもう

「アプリ」段階ではなく
「研究基盤」段階

です。

この実験トラッカーを入れるだけで、

仮説駆動改善

再現性担保

精度の積み上げ

が可能になります。

必要であれば次は：

🔬 自動ハイパーパラメータ探索版

📈 グラフ可視化追加

🧠 Retrieval単体評価分離設計

どこまで深掘りますか？

重要箇所抜水依頼

自動ハイパーパラメータ探索版

📈 グラフ可視化追加

🧠 Retrieval単体評価分離設計

承知しました。
設計の核だけに絞ります。

① 自動ハイパーパラメータ探索（最小・実戦仕様）
🎯 目的

chunk / search_k / rerank_top_k を自動探索

最良構成を特定

実験ログに統合

🔥 探索対象（絞るのが重要）

まずはこれだけで十分：

CHUNK_SIZE      = [300, 350, 400]
SEARCH_K        = [10, 20]
RERANK_TOP_K    = [3, 5]

3 × 2 × 2 = 12通り
CPUでも回せる現実的サイズ。

📁 追加ファイル
rag/evaluation/tune.py
🧠 核となるコード（重要部のみ）
import itertools
import os
from rag.evaluation.evaluate import run_evaluation
from rag.evaluation.experiment import save_experiment

CHUNK_SIZES = [300, 350, 400]
SEARCH_KS = [10, 20]
RERANK_TOP_KS = [3, 5]

def run_grid_search():

    for chunk, search_k, rerank_k in itertools.product(
        CHUNK_SIZES, SEARCH_KS, RERANK_TOP_KS
    ):
        os.environ["CHUNK_SIZE"] = str(chunk)
        os.environ["SEARCH_K"] = str(search_k)
        os.environ["RERANK_TOP_K"] = str(rerank_k)

        result = run_evaluation()

        config = {
            "chunk_size": chunk,
            "search_k": search_k,
            "rerank_top_k": rerank_k,
        }

        save_experiment(config, result)
🔑 重要ポイント

環境変数上書きで再起動不要

evaluate既存ロジックを再利用

save_experimentで履歴統合

Makefile追加
tune:
	python -m rag.evaluation.tune
② 📈 グラフ可視化追加（最小構成）
🎯 目的

精度推移を視覚化

改善が「見える化」される

追加ファイル
rag/evaluation/plot.py
核コード
import matplotlib.pyplot as plt
from rag.evaluation.experiment import load_history

def plot_metrics():

    history = load_history()

    retrieval = [h["metrics"]["retrieval_at_k"] for h in history]
    faith = [h["metrics"]["faithfulness"] for h in history]

    plt.plot(retrieval, label="Retrieval@k")
    plt.plot(faith, label="Faithfulness")

    plt.legend()
    plt.xlabel("Experiment Index")
    plt.ylabel("Score")
    plt.title("RAG Improvement Curve")

    plt.show()
Makefile
plot:
	python -m rag.evaluation.plot
③ 🧠 Retrieval単体評価分離設計（重要）
🎯 なぜ必要？

今は：

Retrieval + Generate 混合評価

これだと：

検索が悪いのか

LLMが悪いのか

切り分け不能。

🔥 分離設計
A. Retrieval評価専用パイプライン
rag/evaluation/retrieval_eval.py
核コード
from rag.pipeline.retrieval import TwoStageRetrieval
from rag.evaluation.metrics import retrieval_at_k
from rag.data.eval_loader import load_questions

def evaluate_retrieval(container):

    retriever = container.retrieval_strategy
    questions = load_questions()

    hits = 0

    for q in questions:
        docs = retriever.retrieve(q["query"])
        sources = [d.metadata["source"] for d in docs]

        if q["expected_source"] in sources:
            hits += 1

    return hits / len(questions)
🔥 重要な分離構造
evaluate_llm.py        → 生成精度
retrieval_eval.py      → 検索精度
evaluate.py            → 統合評価
なぜこれが強いか

これで：

Retrieval@k = 0.95
Faithfulness = 0.72

のように分離可能。

改善戦略が明確になります。

📂 最終ディレクトリ構成（追加分のみ）
rag/
 └── evaluation/
      ├── experiment.py
      ├── compare.py
      ├── tune.py
      ├── plot.py
      ├── retrieval_eval.py
🔥 改善基盤の完成形
make tune        ← 自動探索
make evaluate    ← 通常評価
make compare     ← 差分表示
make plot        ← 推移可視化
本質まとめ

あなたのRAGはここまで来ると：

「RAGアプリ」ではなく
「RAG研究基盤」

になります。

次の段階はどれを攻めますか？

🔬 Bayesian最適化版

🧠 Embedding比較自動化

⚡ MMR vs cosine 自動比較

📊 精度×レイテンシ最適点探索

戦略レベルで設計しますか？