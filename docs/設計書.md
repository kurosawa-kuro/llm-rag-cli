# CLI型 RAG システム 設計書

## 1. システム概要

PDF および CSV ファイルを取り込み、ベクトル検索と Re-ranking を経て、ローカル LLM で日本語回答を生成する CLI 型 RAG（Retrieval-Augmented Generation）システム。

外部 API に依存せず、CPU 環境のみで完結する。フレームワークとして LangChain / LangGraph を採用し、ワークフローをグラフ構造で管理する。DI Container（`AppContainer`）により依存性を一元管理し、Protocol ベースのインターフェースで型安全性を保証する。

---

## 2. アーキテクチャ

### 2.1 全体構成

```text
data/pdf/ ─┐
           ├─→ ingest.py ─→ chunking ─→ embedding ─→ PostgreSQL + pgvector
data/csv/ ─┘                                              │
                                                          ↓
                ask.py ←── TwoStageRetrieval ←── ベクトル検索 (k=10)
                  │              └── reranker ←── Cross-Encoder (top_k=3)
                  ↓
           graph.py (LangGraph)
           ┌────────────────────────┐
           │ retrieve → generate    │
           └────────────────────────┘
                  │
                  ↓
             LLM回答 + ソース出力
```

### 2.2 Two-Stage Retrieval

検索精度を高めるため、2段階の検索パイプラインを採用する。`TwoStageRetrieval` クラスが2段階検索をカプセル化する。

1. **第1段階（Vector Search）**: pgvector のコサイン距離検索で候補を SEARCH_K 件（デフォルト10件）取得
2. **第2段階（Re-ranking）**: Cross-Encoder で候補をスコアリングし直し、上位 RERANK_TOP_K 件（デフォルト3件）に絞り込み

### 2.3 LangGraph ワークフロー

ask.py の main 関数は LangGraph の StateGraph を使用してパイプラインを実行する。2ノード構成で、retrieve ノードが `TwoStageRetrieval` を呼び出すことでベクトル検索とリランキングを一括実行する。

**状態オブジェクト（RAGState）の構成:**

| フィールド | 型 | 説明 |
|---|---|---|
| query | str | ユーザーの質問文 |
| reranked_documents | List[Document] | TwoStageRetrieval による検索+リランキング後の文書 |
| contexts | List[str] | LLM に渡すコンテキスト文字列 |
| prompt | str | 組み立てられたプロンプト全文 |
| answer | str | LLM の生成回答 |
| sources | List[str] | 回答根拠の出典一覧 |

**ノード遷移:**

```text
[retrieve] → [generate] → END
```

| ノード名 | 処理内容 | 入力状態キー | 出力状態キー |
|---|---|---|---|
| retrieve | TwoStageRetrieval で検索+リランキングを実行（SEARCH_K件 → RERANK_TOP_K件） | query | reranked_documents |
| generate | 日本語プロンプトを組み立て LLM で回答生成 | reranked_documents, query | contexts, prompt, answer, sources |

### 2.4 DI Container パターン

`AppContainer` が全インフラ依存を lazy property で管理し、テスト容易性と疎結合を実現する。

```text
AppContainer
├── embeddings          → create_embeddings()
├── vectorstore         → create_vectorstore(embeddings)
├── reranker            → create_reranker()
├── llm                 → create_llm()
├── prompt_builder      → build_prompt
└── retrieval_strategy  → TwoStageRetrieval(vectorstore, reranker, ...)
```

- インフラモジュール（`db.py`, `embeddings.py`, `llm.py`, `reranker.py`）はステートレスなファクトリ関数（`create_*`）のみ提供
- `interfaces.py` の Protocol クラスにより型安全な依存注入を実現
- テスト時はコンストラクタ引数でモックを注入可能

---

## 3. ディレクトリ構成

```text
llm-rag-cli/
├── src/
│   ├── rag/
│   │   ├── __init__.py
│   │   ├── core/                 コア（設定・インターフェース・DI）
│   │   │   ├── __init__.py
│   │   │   ├── config.py         設定・定数管理（setting.yaml ベース）
│   │   │   ├── interfaces.py     Protocol 定義（型安全なインターフェース）
│   │   │   └── container.py      DI Container（AppContainer + RagSettings）
│   │   ├── pipeline/             パイプライン（グラフ・検索戦略）
│   │   │   ├── __init__.py
│   │   │   ├── graph.py          LangGraphワークフロー（2ノード）
│   │   │   └── retrieval.py      検索戦略（TwoStageRetrieval）
│   │   ├── components/           コンポーネント（モデル・プロンプト）
│   │   │   ├── __init__.py
│   │   │   ├── embeddings.py     テキスト埋め込みファクトリ
│   │   │   ├── reranker.py       Re-ranking ファクトリ
│   │   │   ├── llm.py            LLM推論ファクトリ
│   │   │   └── prompting.py      プロンプト構築
│   │   ├── data/                 データ取り込み
│   │   │   ├── __init__.py
│   │   │   ├── chunking.py       テキスト分割
│   │   │   └── ingest.py         データ取り込みパイプライン
│   │   ├── evaluation/           評価
│   │   │   ├── __init__.py
│   │   │   ├── metrics.py        評価指標
│   │   │   └── evaluate.py       評価パイプライン
│   │   └── infra/                インフラ（DB接続）
│   │       ├── __init__.py
│   │       └── db.py             ベクトルDB接続ファクトリ
│   └── cli/                      CLIエントリポイント
│       ├── __init__.py
│       └── ask.py                質問応答CLI
├── tests/
│   ├── conftest.py               共通フィクスチャ
│   └── test_*.py                 各モジュールのテスト（17ファイル）
├── data/
│   ├── pdf/                      PDF格納ディレクトリ
│   ├── csv/                      CSV格納ディレクトリ
│   └── eval_questions.json       評価用質問セット
├── models/                       LLMモデル格納ディレクトリ
├── env/
│   ├── config/
│   │   └── setting.yaml          アプリケーション設定（DB・モデル・パラメータ）
│   └── secret/                   シークレット配置先
├── docs/
│   ├── 設計書.md                  本ファイル
│   ├── リファクタリング.md         リファクタリング記録
│   └── 実Embeddings統合テストの最小.md  実Embeddings統合テストガイド
├── docker-compose.yml
├── Dockerfile
├── Makefile
├── requirements.txt
├── requirements-dev.txt
└── pytest.ini
```

---

## 4. モジュール仕様

### 4.1 config.py (src/rag/core/) — 設定管理

`env/config/setting.yaml` から設定を読み込み、環境変数でオーバーライド可能。数値パラメータ（CHUNK_SIZE, CHUNK_OVERLAP, SEARCH_K, RERANK_TOP_K）は環境変数が優先される。

**関数:**

| 関数名 | 引数 | 戻り値 | 説明 |
|---|---|---|---|
| _load_settings | なし | dict | setting.yaml を読み込み、設定辞書を返却 |
| get_db_config | なし | dict | 環境変数（優先）またはYAMLからDB接続情報を取得。未設定時はYAMLのデフォルト値を使用 |
| get_connection_string | なし | str | LangChain用 PostgreSQL 接続文字列を生成。形式は postgresql+psycopg://user:pass@host:port/dbname |

**定数:**

| 定数名 | 値 | 説明 |
|---|---|---|
| EMBED_MODEL | YAMLの models.embed_model | 埋め込みモデル（384次元） |
| LLM_MODEL_PATH | YAMLの models.llm_model_path | LLMモデルファイルパス |
| RERANKER_MODEL | YAMLの models.reranker_model | Re-rankingモデル |
| CHUNK_SIZE | 500（環境変数またはYAMLで変更可） | チャンク分割サイズ（文字数） |
| CHUNK_OVERLAP | 100（環境変数またはYAMLで変更可） | チャンク間オーバーラップ（文字数） |
| SEARCH_K | 10（環境変数またはYAMLで変更可） | ベクトル検索の取得件数 |
| RERANK_TOP_K | 3（環境変数またはYAMLで変更可） | Re-ranking後の採用件数 |
| COLLECTION_NAME | YAMLの collection_name | pgvector のコレクション名 |
| LLM_N_CTX | YAMLの llm.n_ctx | LLM コンテキストウィンドウサイズ |
| LLM_MAX_TOKENS | YAMLの llm.max_tokens | LLM 最大生成トークン数 |

### 4.2 interfaces.py (src/rag/core/) — Protocol 定義

DI Container の型安全性を保証する Protocol クラス群。

| Protocol | メソッド | 説明 |
|---|---|---|
| RetrieverProtocol | `invoke(query: str) -> List[Document]` | 検索結果を返すリトリーバー |
| VectorStoreProtocol | `as_retriever(*, search_kwargs: dict) -> RetrieverProtocol` | リトリーバーを生成するベクトルストア |
| RerankerProtocol | `compress_documents(documents, query) -> List[Document]` | ドキュメントをリランキング |
| LLMProtocol | `invoke(prompt: str) -> str` | プロンプトから回答を生成 |
| RetrievalStrategyProtocol | `retrieve(query: str) -> List[Document]` | 検索戦略の抽象 |
| PromptBuilder | `Callable[[str, List[str]], str]` | プロンプト構築関数の型エイリアス |

### 4.3 container.py (src/rag/core/) — DI Container

`AppContainer` が全インフラ依存を lazy property で管理し、`get_container()` でシングルトンを返却する。

**クラス:**

| クラス名 | 説明 |
|---|---|
| RagSettings | `@dataclass(frozen=True)`。`search_k`, `rerank_top_k` を保持 |
| AppContainer | DI Container。全依存をコンストラクタ引数（オプション）または lazy property で管理 |

**AppContainer の property:**

| property | 型 | 生成元 |
|---|---|---|
| embeddings | HuggingFaceEmbeddings | `create_embeddings()` |
| vectorstore | VectorStoreProtocol | `create_vectorstore(self.embeddings)` |
| reranker | RerankerProtocol | `create_reranker()` |
| llm | LLMProtocol | `create_llm()` |
| prompt_builder | PromptBuilder | `build_prompt` |
| retrieval_strategy | RetrievalStrategyProtocol | `TwoStageRetrieval(vectorstore, reranker, search_k, rerank_top_k)` |

### 4.4 retrieval.py (src/rag/pipeline/) — 検索戦略

`TwoStageRetrieval` が2段階検索をカプセル化する。

**クラス:**

| クラス名 | 説明 |
|---|---|
| TwoStageRetrieval | `@dataclass(frozen=True)`。`vectorstore`, `reranker`, `search_k`, `rerank_top_k` を保持。`retrieve(query)` でベクトル検索 → リランキングを実行 |

**処理フロー:**

1. `vectorstore.as_retriever(search_kwargs={"k": search_k})` でリトリーバーを生成
2. `retriever.invoke(query)` でベクトル検索（SEARCH_K件）
3. `reranker.compress_documents(docs, query)` でリランキング
4. 上位 `rerank_top_k` 件を返却

### 4.5 db.py (src/rag/infra/) — ベクトルDB接続

LangChain の PGVector ラッパーを介して PostgreSQL + pgvector に接続するファクトリ関数を提供。

**関数:**

| 関数名 | 説明 |
|---|---|
| create_vectorstore(embeddings) | `PGVector` インスタンスを生成して返却。`CONNECTION_STRING`, `COLLECTION_NAME`, `use_jsonb=True` |

**接続仕様:**
- ドライバ: psycopg（psycopg3系）
- メタデータ格納: JSONB（use_jsonb=True）
- ベクトル次元数: 384（埋め込みモデルに依存）

### 4.6 embeddings.py (src/rag/components/) — テキスト埋め込み

LangChain の HuggingFaceEmbeddings ラッパーを使用するファクトリ関数を提供。

**関数:**

| 関数名 | 引数 | 戻り値 | 説明 |
|---|---|---|---|
| create_embeddings | なし | HuggingFaceEmbeddings | 埋め込みモデルインスタンスを生成して返却 |
| embed | texts: list[str] | list[list[float]] | テキストリストを384次元ベクトルリストに変換 |

**モデル仕様:**
- モデル: all-MiniLM-L6-v2
- 出力次元: 384
- 対応言語: 多言語（日本語含む）

### 4.7 llm.py (src/rag/components/) — LLM推論

LangChain の LlamaCpp ラッパーを使用するファクトリ関数を提供。GGUF 量子化モデルを CPU で推論する。

**関数:**

| 関数名 | 引数 | 戻り値 | 説明 |
|---|---|---|---|
| create_llm | なし | LlamaCpp | LLMインスタンスを生成して返却 |
| generate | prompt: str | str | プロンプトを受け取り、LLMの生成テキストを返す |

**LLMパラメータ:**

| パラメータ | 値 | 説明 |
|---|---|---|
| model_path | ./models/llama-2-7b.Q4_K_M.gguf | GGUF形式のモデルファイル |
| n_ctx | 2048 | コンテキストウィンドウサイズ |
| max_tokens | 300 | 最大生成トークン数 |
| verbose | False | 推論ログ出力の抑制 |

### 4.8 reranker.py (src/rag/components/) — Re-ranking

Cross-Encoder によるスコアリングで検索結果を再順位付けするファクトリ関数を提供。LangChain の ContextualCompressionRetriever パターンも対応。

**関数:**

| 関数名 | 引数 | 戻り値 | 説明 |
|---|---|---|---|
| create_reranker | なし | CrossEncoderReranker | Re-rankerインスタンスを生成して返却。top_n は RERANK_TOP_K に従う |
| get_compression_retriever | base_retriever | ContextualCompressionRetriever | ベースRetrieverをRe-rankerでラップしたRetrieverを返す |
| rerank | query, docs, top_k=3 | list[dict] | dict形式のドキュメントリストを受け取り、Re-ranking後のリストを返す。空リスト・None入力時は空リストを返す |

**Re-rankerモデル仕様:**
- モデル: ms-marco-MiniLM-L-6-v2（Cross-Encoder）
- 動作: CPU対応
- 入力: (query, document) ペアに対しスコアを算出

### 4.9 chunking.py (src/rag/data/) — テキスト分割

外部ライブラリに依存しない独自実装のチャンク分割モジュール。

**関数:**

| 関数名 | 引数 | 戻り値 | 説明 |
|---|---|---|---|
| split_text | text, chunk_size=500, overlap=100 | list[str] | 固定サイズ分割。単語境界（空白）で分割位置を調整し、overlap分の重複を持たせる。None・空文字は空リストを返す |
| split_by_structure | text, chunk_size=None, overlap=100 | list[str] | 段落ベース分割。連続改行（\\n\\n）で段落に分割後、chunk_size指定時は長い段落をさらにsplit_textで分割する。chunk_size=None時は段落をそのまま返す |

**分割ルール:**
- split_text: テキスト長 <= chunk_size の場合は分割しない。分割位置は後方最寄りの空白を優先し、空白がない場合は前方に拡張。チャンク間の重複により文脈の断絶を防止
- split_by_structure: 空の段落（空白のみ）は除外。各段落の前後空白は除去

### 4.10 prompting.py (src/rag/components/) — プロンプト構築

日本語プロンプトテンプレートを構築するモジュール。

**関数:**

| 関数名 | 引数 | 戻り値 | 説明 |
|---|---|---|---|
| build_prompt | query: str, contexts: list[str] | str | コンテキストと質問を埋め込んだ日本語プロンプトを構築 |

### 4.11 ingest.py (src/rag/data/) — データ取り込みパイプライン

PDF と CSV を読み込み、チャンク分割・メタデータ付与のうえベクトルDBに一括格納する。

**関数:**

| 関数名 | 説明 |
|---|---|
| load_pdfs | data/pdf/ 配下の全PDFファイルをページ単位で読み込み、(テキスト, ソース) のタプルリストを返す |
| load_csvs | data/csv/ 配下の全CSVファイルを行単位で読み込み、各行を "カラム名:値" 形式のテキストに変換して返す |
| main | `get_container()` → PDF/CSV読み込み → チャンク分割 → メタデータ付与 → `container.vectorstore.add_documents()` で格納 |

**ソースID命名規則:**

| データ種別 | 形式 | 例 |
|---|---|---|
| PDF | ファイル名:p{ページ番号} | company_overview.pdf:p1 |
| CSV | ファイル名:r{行番号} | faq.csv:r1 |

※ページ番号・行番号はともに1始まり

**分割戦略の使い分け:**

| データ種別 | 分割方式 | 理由 |
|---|---|---|
| PDF | split_by_structure（段落ベース） | 文書構造（段落・セクション）を活かした分割 |
| CSV | RecursiveCharacterTextSplitter（LangChain） | 行単位テキストの固定サイズ分割 |

**メタデータ:**

各 Document オブジェクトに以下のメタデータを付与する。

| キー | 型 | 説明 |
|---|---|---|
| source | str | ソースID（上記命名規則に従う） |
| chunk_index | int | 同一ソース内でのチャンク連番（0始まり） |

### 4.12 ask.py (src/cli/) — 質問応答 CLI

ユーザーの質問をコマンドライン引数で受け取り、LangGraph ワークフローを実行して回答とソースを出力する。

**関数:**

| 関数名 | 説明 |
|---|---|
| main | `sys.argv[1]` から質問を取得し、`get_container()` → `get_graph(container=...)` で LangGraph ワークフローを構築・実行。回答とソース一覧を標準出力に表示 |

**出力フォーマット:**

```text
=== Answer ===
{LLMの生成回答}

=== Sources ===
- {ソースID1}
- {ソースID2}
```

### 4.13 graph.py (src/rag/pipeline/) — LangGraph ワークフロー定義

RAG パイプラインを LangGraph の StateGraph として定義する。2ノード構成で、retrieve ノードが `TwoStageRetrieval` を呼び出すことでベクトル検索+リランキングを一括実行する。

**関数:**

| 関数名 | 説明 |
|---|---|
| create_retrieve(container) | `container.retrieval_strategy.retrieve()` を呼び出す retrieve ノード関数を生成。検索+リランキング後の文書を `reranked_documents` に格納 |
| create_generate(container) | reranked_documents からコンテキストとソースを抽出し、日本語プロンプトを組み立てて LLM を呼び出す generate ノード関数を生成 |
| build_rag_graph(*, container=None) | 2ノード StateGraph（retrieve → generate → END）を構築・コンパイルして返す |
| get_graph(*, container=None) | container 指定時は毎回新規構築、未指定時はシングルトン返却 |

**プロンプトテンプレート:**

```text
以下の情報を基に回答してください:

{コンテキストリスト}

質問:{ユーザーの質問}
回答:
```

### 4.14 metrics.py (src/rag/evaluation/) — 評価指標

RAG パイプラインの精度・品質を定量評価するための指標関数群。

**関数:**

| 関数名 | 引数 | 戻り値 | 説明 |
|---|---|---|---|
| retrieval_at_k | results, expected_source | bool | 検索結果リスト内に期待するソースIDが含まれるか判定。完全一致で比較 |
| faithfulness | answer, expected_keywords | float (0.0〜1.0) | 期待キーワードのうち回答に含まれるものの割合。キーワード空時は1.0 |
| exact_match | answer, expected_keywords | bool | 全キーワードが回答に含まれるか判定。キーワード空時はTrue |
| measure_latency | func | (result, float) | 関数の実行結果と経過秒数のタプルを返す |

### 4.15 evaluate.py (src/rag/evaluation/) — 評価パイプライン

評価用質問セットに対してパイプラインを実行し、精度レポートを出力する。

**関数:**

| 関数名 | 説明 |
|---|---|
| load_questions | data/eval_questions.json を読み込み、質問リストを返す |
| evaluate_single | 1問分の評価を実行。`graph.invoke()` で回答生成 → 4指標（retrieval_hit, faithfulness, exact_match, latency）を算出 |
| run_evaluation | 全質問に対して evaluate_single を実行し、結果リストを返す |
| print_report | 設定情報と集計結果（Retrieval@k, Faithfulness, Exact Match, Latency）をフォーマット出力 |
| main | `get_container()` → `get_graph(container=...)` → 全問評価 → レポート出力 |

**評価用質問セット（data/eval_questions.json）の構造:**

| フィールド | 型 | 説明 |
|---|---|---|
| query | str | 評価用の質問文 |
| expected_source | str | 正解のソースID（命名規則に従う） |
| expected_keywords | list[str] | 回答に含まれるべきキーワード |

現在13問を収録。CSV（FAQ・商品）および PDF（会社概要・技術ガイド）から出題。

**レポート出力フォーマット:**

```text
=== Evaluation Report ===

Chunk=500 overlap=100
Top-k=10
Re-rank=ON (top_k=3)

Retrieval@k: xx.x%
Faithfulness: xx.x%
Exact Match: xx.x%
Latency: xx.xs

Questions evaluated: 13
```

---

## 5. データベース仕様

### 5.1 構成

| 項目 | 値 |
|---|---|
| RDBMS | PostgreSQL 16 |
| 拡張 | pgvector |
| イメージ | pgvector/pgvector:pg16 |
| 接続ドライバ | psycopg（psycopg3系） |
| ORM/ラッパー | LangChain PGVector（langchain-postgres） |

### 5.2 データモデル

LangChain PGVector が管理するスキーマを使用。ユーザーが直接テーブルを定義する必要はない。

**主要カラム（LangChain PGVector 内部）:**

| カラム | 型 | 説明 |
|---|---|---|
| id | UUID | ドキュメント一意識別子 |
| collection_id | UUID | コレクションへの外部キー |
| document | TEXT | チャンクテキスト本文 |
| embedding | VECTOR(384) | 384次元埋め込みベクトル |
| cmetadata | JSONB | メタデータ（source, chunk_index） |

---

## 6. チャンク分割仕様

### 6.1 分割戦略

| 戦略 | 対象 | 分割単位 | 実装 |
|---|---|---|---|
| 段落ベース分割 | PDF | 連続改行（\\n\\n）で段落分割後、長い段落は固定サイズで再分割 | chunking.split_by_structure |
| 固定サイズ分割 | CSV | LangChain の RecursiveCharacterTextSplitter | langchain_text_splitters |

### 6.2 パラメータ

| パラメータ | デフォルト値 | 設定元 | 説明 |
|---|---|---|---|
| chunk_size | 500文字 | 環境変数 `CHUNK_SIZE` または setting.yaml | 1チャンクの最大文字数 |
| chunk_overlap | 100文字 | 環境変数 `CHUNK_OVERLAP` または setting.yaml | チャンク間の重複文字数 |

### 6.3 分割時の制約

- 単語境界（空白）で分割位置を調整し、単語の途中で切断しない
- overlap が chunk_size 以上の場合でも無限ループにならない安全設計
- 空テキスト・None 入力時は空リストを返す
- 空白のみの段落は除外される

---

## 7. 環境変数一覧

| 変数名 | デフォルト値 | 型 | 説明 |
|---|---|---|---|
| DB_HOST | localhost | str | PostgreSQL ホスト名（Docker内では db） |
| DB_USER | rag | str | PostgreSQL ユーザー名 |
| DB_PASSWORD | rag | str | PostgreSQL パスワード |
| DB_NAME | rag | str | PostgreSQL データベース名 |
| CHUNK_SIZE | 500 | int | チャンク分割サイズ（文字数） |
| CHUNK_OVERLAP | 100 | int | チャンク間オーバーラップ（文字数） |
| SEARCH_K | 10 | int | ベクトル検索の取得件数 |
| RERANK_TOP_K | 3 | int | Re-ranking後の採用件数 |

※ デフォルト値は `env/config/setting.yaml` で定義。環境変数が設定されている場合は環境変数が優先される。
※ 数値型の環境変数に非数値を設定した場合は ValueError が発生する。

---

## 8. Docker 構成

### 8.1 サービス構成

| サービス | イメージ | コンテナ名 | ポート | 説明 |
|---|---|---|---|---|
| db | pgvector/pgvector:pg16 | rag_db | 5432:5432 | PostgreSQL + pgvector |
| app | ローカルビルド | rag_app | なし | Python アプリケーション |

### 8.2 app コンテナ仕様

| 項目 | 値 |
|---|---|
| ベースイメージ | python:3.11-slim |
| 作業ディレクトリ | /app |
| ボリュームマウント | ホストのプロジェクトルート → /app |
| PYTHONPATH | src |
| システム依存パッケージ | build-essential, git, curl, poppler-utils |
| DB接続 | 環境変数で DB_HOST=db を設定（Docker内部DNS） |

### 8.3 永続化

PostgreSQL データは名前付きボリューム pgdata で永続化。アプリケーションコンテナはボリュームマウントによりホストとコード共有。

---

## 9. 依存ライブラリ

### 9.1 コアライブラリ

| ライブラリ | バージョン要件 | 用途 |
|---|---|---|
| langchain | >= 0.3.0 | RAGフレームワーク基盤 |
| langchain-core | >= 0.3.0 | Document, Retriever 等の基本型 |
| langchain-community | >= 0.3.0 | LlamaCpp, HuggingFaceCrossEncoder |
| langchain-huggingface | >= 0.1.0 | HuggingFaceEmbeddings |
| langchain-postgres | >= 0.0.12 | PGVector ラッパー |
| langchain-text-splitters | >= 0.3.0 | RecursiveCharacterTextSplitter |
| langgraph | >= 0.2.0 | StateGraph によるワークフロー管理 |

### 9.2 ML / データ処理

| ライブラリ | 用途 |
|---|---|
| sentence-transformers | 埋め込みモデルのバックエンド |
| llama-cpp-python | GGUF形式 LLM の CPU推論 |
| pypdf | PDF テキスト抽出 |
| pandas | CSV 読み込み・行処理 |
| numpy | ベクトル演算 |

### 9.3 DB接続

| ライブラリ | 用途 |
|---|---|
| psycopg2-binary | PostgreSQL 接続（レガシー互換） |
| psycopg[binary] >= 3.1.0 | PostgreSQL 接続（langchain-postgres 用） |

### 9.4 設定・ユーティリティ

| ライブラリ | 用途 |
|---|---|
| pyyaml | YAML 設定ファイル読み込み |
| tqdm | プログレスバー |

---

## 10. CLI インターフェース（Makefile）

| コマンド | 説明 |
|---|---|
| make build | Docker イメージをビルド |
| make up | コンテナをバックグラウンド起動（PostgreSQL + app） |
| make down | コンテナを停止 |
| make shell | app コンテナに bash で入る |
| make test | 全テストを実行（pytest -v） |
| make test-unit | 単体テストのみ実行（DB不要、`-m "not integration and not heavy"`） |
| make test-integration | DB統合テストのみ実行（PostgreSQL必要、heavy除外） |
| make test-heavy | 実Embeddingsテスト実行（PostgreSQL+モデルDL必要、`-m heavy`） |
| make lint | 全 src/ モジュールの構文チェック（py_compile） |
| make ingest | データ取り込みパイプラインを実行（`python -m rag.data.ingest`） |
| make ask Q="質問文" | RAG に質問して回答を取得（`python -m cli.ask`） |
| make evaluate | 評価パイプラインを実行しレポート出力（`python -m rag.evaluation.evaluate`） |

全コマンドは PYTHONPATH=src のもと python -m 形式で実行し、モジュールインポートの整合性を保証する。

---

## 11. DI Container パターン

`AppContainer`（`src/rag/core/container.py`）が重量級リソース（モデルロード・DB接続）を lazy property で管理する。初回アクセス時に遅延読み込みし、以降は同一インスタンスを再利用する。

**AppContainer の依存管理:**

| property | 管理対象 | 生成元ファクトリ |
|---|---|---|
| embeddings | HuggingFaceEmbeddings インスタンス | `rag.components.embeddings.create_embeddings()` |
| vectorstore | PGVector インスタンス | `rag.infra.db.create_vectorstore(embeddings)` |
| reranker | CrossEncoderReranker インスタンス | `rag.components.reranker.create_reranker()` |
| llm | LlamaCpp インスタンス | `rag.components.llm.create_llm()` |
| prompt_builder | プロンプト構築関数 | `rag.components.prompting.build_prompt` |
| retrieval_strategy | TwoStageRetrieval インスタンス | `rag.pipeline.retrieval.TwoStageRetrieval(...)` |

**シングルトン管理:**

| モジュール | グローバル変数 | 説明 |
|---|---|---|
| rag.core.container | `_container` | AppContainer のシングルトン |
| rag.pipeline.graph | `_graph` | コンパイル済み StateGraph のシングルトン（container 未指定時のみ） |

テスト時は `AppContainer` のコンストラクタ引数でモックを注入し、`conftest.py` の `reset_container` fixture で `_container = None` にリセットする。

---

## 12. テスト方針

### 12.1 基本方針

- テストフレームワーク: pytest + pytest-mock
- モック戦略: sentence-transformers, llama-cpp-python, psycopg 等の重量級依存はすべてモック化。テスト実行に DB・モデルファイルは不要（単体テスト）
- テスト構成: pytest.ini で pythonpath = src を設定し、`from rag.xxx import ...` / `from cli.xxx import ...` 形式のインポートを使用
- DI Container: テスト時は `AppContainer` のコンストラクタ引数でモックを注入

### 12.2 テスト分類

| 分類 | マーカー | 目的 | 例 |
|---|---|---|---|
| 単体テスト | なし | 期待する入出力の検証（モック使用） | main が graph.invoke を正しく呼び出す |
| DB統合テスト | `@pytest.mark.integration` | 実DB接続での動作検証 | PGVector への書き込み・検索 |
| Heavy テスト | `@pytest.mark.heavy` | 実モデルでの動作検証 | 実HuggingFaceEmbeddings でのベクトル検索 |
| 正常系 | — | 期待する入出力の検証 | search が content/source の dict リストを返す |
| 異常系 | — | エラー条件でのエラー種別の固定 | 存在しないファイルで FileNotFoundError |
| バリデーション | — | 境界値・不正入力への振る舞い固定 | None入力, 空リスト, sourceキー欠損 |

### 12.3 共通フィクスチャ（conftest.py）

| フィクスチャ名 | 説明 |
|---|---|
| mock_db_connection | psycopg2 のコンテキストマネージャモック |
| fake_embeddings | 3×384 の NumPy 配列 |
| mock_llm_response | LLM レスポンスの dict モック |
| mock_vectorstore | LangChain vectorstore モック（as_retriever 対応） |
| mock_documents | source/chunk_index メタデータ付き Document リスト |
| reset_container | autouse: AppContainer シングルトン（`_container`）を各テスト前後でリセット |
| test_embeddings | FakeEmbeddings（384次元、統合テスト用） |
| test_vectorstore | 実PGVector（test_documents コレクション、統合テスト用） |
| real_vectorstore | 実HuggingFaceEmbeddings + 実PGVector（heavy テスト用） |

### 12.4 テスト規模

| テストファイル | 対象モジュール | テスト数 |
|---|---|---|
| test_config.py | rag.core.config | 25 |
| test_interfaces.py | rag.core.interfaces | 5 |
| test_container.py | rag.core.container | 21 |
| test_retrieval.py | rag.pipeline.retrieval | 6 |
| test_db.py | rag.infra.db | 6 |
| test_db_integration.py | rag.infra.db (統合) | 7 |
| test_embeddings.py | rag.components.embeddings | 9 |
| test_embeddings_integration.py | rag.components.embeddings (heavy) | 3 |
| test_llm.py | rag.components.llm | 9 |
| test_chunking.py | rag.data.chunking | 32 |
| test_reranker.py | rag.components.reranker | 18 |
| test_prompting.py | rag.components.prompting | 6 |
| test_ingest.py | rag.data.ingest | 23 |
| test_ask.py | cli.ask | 8 |
| test_graph.py | rag.pipeline.graph | 19 |
| test_metrics.py | rag.evaluation.metrics | 38 |
| test_evaluate.py | rag.evaluation.evaluate | 43 |
| **合計** | **17ファイル** | **280** |

---

## 13. LLM モデル配置

| 項目 | 値 |
|---|---|
| モデル | Llama-2-7B |
| 量子化 | Q4_K_M（GGUF形式） |
| 配置先 | models/llama-2-7b.Q4_K_M.gguf |
| 推論方式 | CPU（llama-cpp-python経由） |
| 推奨環境 | CPU で Q4_K_M 量子化版を使用 |

---

## 14. データフロー詳細

### 14.1 取り込みフロー（make ingest）

```text
1. get_container()  →  AppContainer 取得（rag.core.container）
2. load_pdfs()  →  data/pdf/*.pdf をページ単位で読み込み（rag.data.ingest）
3. load_csvs()  →  data/csv/*.csv を行単位で読み込み（rag.data.ingest）
4. PDF: split_by_structure  →  段落ベースでチャンク分割（rag.data.chunking）
5. CSV: RecursiveCharacterTextSplitter  →  固定サイズでチャンク分割
6. 各チャンクに source, chunk_index メタデータを付与
7. container.vectorstore.add_documents()  →  一括格納（埋め込み生成はPGVector内部で実行）
```

### 14.2 質問応答フロー（make ask Q="..."）

```text
1. sys.argv[1] からクエリを取得（cli.ask）
2. get_container() → get_graph(container=...) で LangGraph ワークフローを構築（rag.pipeline.graph）
3. graph.invoke() で 2ノードパイプラインを実行:
   a. [retrieve]  TwoStageRetrieval.retrieve() を実行（rag.pipeline.retrieval）
      - vectorstore で SEARCH_K=10 件をベクトル検索
      - reranker で RERANK_TOP_K=3 件にリランキング
   b. [generate]  日本語プロンプト組立（rag.components.prompting） → LLM生成（rag.components.llm）
4. 回答テキストとソースIDを標準出力に表示
```

### 14.3 評価フロー（make evaluate）

```text
1. data/eval_questions.json から質問セットを読み込み（rag.evaluation.evaluate）
2. get_container() → get_graph(container=...) でグラフ構築（rag.pipeline.graph）
3. 各質問に対して:
   a. graph.invoke()  →  検索 + リランキング + LLM回答生成
   b. retrieval_at_k  →  正解ソースが検索結果に含まれるか（rag.evaluation.metrics）
   c. faithfulness  →  キーワード含有率（rag.evaluation.metrics）
   d. exact_match  →  全キーワードの含有判定（rag.evaluation.metrics）
   e. measure_latency  →  処理時間計測（rag.evaluation.metrics）
4. 全質問の集計結果をレポート出力
```
