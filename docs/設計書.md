# CLI型 RAG システム 設計書

## 1. システム概要

PDF および CSV ファイルを取り込み、ベクトル検索と Re-ranking を経て、ローカル LLM で日本語回答を生成する CLI 型 RAG（Retrieval-Augmented Generation）システム。

外部 API に依存せず、CPU 環境のみで完結する。フレームワークとして LangChain / LangGraph を採用し、ワークフローをグラフ構造で管理する。

---

## 2. アーキテクチャ

### 2.1 全体構成

```text
data/pdf/ ─┐
           ├─→ ingest.py ─→ chunking ─→ embedding ─→ PostgreSQL + pgvector
data/csv/ ─┘                                              │
                                                          ↓
                ask.py ←── reranker ←── ベクトル検索 (k=10)
                  │
                  ↓
           graph.py (LangGraph)
           ┌─────────────────────────┐
           │ retrieve → rerank → generate │
           └─────────────────────────┘
                  │
                  ↓
             LLM回答 + ソース出力
```

### 2.2 Two-Stage Retrieval

検索精度を高めるため、2段階の検索パイプラインを採用する。

1. **第1段階（Vector Search）**: pgvector のコサイン距離検索で候補を SEARCH_K 件（デフォルト10件）取得
2. **第2段階（Re-ranking）**: Cross-Encoder で候補をスコアリングし直し、上位 RERANK_TOP_K 件（デフォルト3件）に絞り込み

### 2.3 LangGraph ワークフロー

ask.py の main 関数は LangGraph の StateGraph を使用してパイプラインを実行する。

**状態オブジェクト（RAGState）の構成:**

| フィールド | 型 | 説明 |
|---|---|---|
| query | str | ユーザーの質問文 |
| documents | List[Document] | ベクトル検索結果 |
| reranked_documents | List[Document] | Re-ranking 後の文書 |
| contexts | List[str] | LLM に渡すコンテキスト文字列 |
| prompt | str | 組み立てられたプロンプト全文 |
| answer | str | LLM の生成回答 |
| sources | List[str] | 回答根拠の出典一覧 |

**ノード遷移:**

```text
[retrieve] → [rerank] → [generate] → END
```

| ノード名 | 処理内容 | 入力状態キー | 出力状態キー |
|---|---|---|---|
| retrieve | pgvector から SEARCH_K 件を取得 | query | documents |
| rerank | Cross-Encoder で RERANK_TOP_K 件に絞り込み | documents, query | reranked_documents |
| generate | 日本語プロンプトを組み立て LLM で回答生成 | reranked_documents, query | contexts, prompt, answer, sources |

---

## 3. ディレクトリ構成

```text
rag-cli/
├── app/
│   ├── __init__.py
│   ├── config.py          設定・定数管理
│   ├── db.py              ベクトルDB接続
│   ├── embeddings.py      テキスト埋め込み
│   ├── llm.py             LLM推論
│   ├── reranker.py        Re-ranking
│   ├── chunking.py        テキスト分割
│   ├── ingest.py          データ取り込み
│   ├── ask.py             質問応答CLI
│   ├── graph.py           LangGraphワークフロー
│   ├── metrics.py         評価指標
│   └── evaluate.py        評価パイプライン
├── tests/
│   ├── conftest.py        共通フィクスチャ
│   └── test_*.py          各モジュールのテスト（11ファイル）
├── data/
│   ├── pdf/               PDF格納ディレクトリ
│   ├── csv/               CSV格納ディレクトリ
│   └── eval_questions.json 評価用質問セット
├── models/                LLMモデル格納ディレクトリ
├── docker-compose.yml
├── Dockerfile
├── Makefile
├── requirements.txt
└── pytest.ini
```

---

## 4. モジュール仕様

### 4.1 config.py — 設定管理

すべての設定値を一元管理する。数値パラメータは環境変数で上書き可能。モデルパス・モデル名は定数として定義。

**関数:**

| 関数名 | 引数 | 戻り値 | 説明 |
|---|---|---|---|
| get_db_config | なし | dict | DB接続情報を環境変数から取得。未設定時はデフォルト値を使用 |
| get_connection_string | なし | str | LangChain用 PostgreSQL 接続文字列を生成。形式は postgresql+psycopg://user:pass@host:5432/dbname |

**定数:**

| 定数名 | 値 | 説明 |
|---|---|---|
| EMBED_MODEL | sentence-transformers/all-MiniLM-L6-v2 | 埋め込みモデル（384次元） |
| LLM_MODEL_PATH | ./models/llama-2-7b.Q4_K_M.gguf | LLMモデルファイルパス |
| RERANKER_MODEL | cross-encoder/ms-marco-MiniLM-L-6-v2 | Re-rankingモデル |
| CHUNK_SIZE | 500（環境変数で変更可） | チャンク分割サイズ（文字数） |
| CHUNK_OVERLAP | 100（環境変数で変更可） | チャンク間オーバーラップ（文字数） |
| SEARCH_K | 10（環境変数で変更可） | ベクトル検索の取得件数 |
| RERANK_TOP_K | 3（環境変数で変更可） | Re-ranking後の採用件数 |
| COLLECTION_NAME | documents | pgvector のコレクション名 |

### 4.2 db.py — ベクトルDB接続

LangChain の PGVector ラッパーを介して PostgreSQL + pgvector に接続する。シングルトンパターンでインスタンスを管理。

**関数:**

| 関数名 | 説明 |
|---|---|
| get_vectorstore | PGVector インスタンスを返す。初回呼び出し時に接続を確立し、以降は同一インスタンスを返す。メタデータは JSONB 形式で格納 |
| init_db | get_vectorstore を呼び出し、DB初期化を行う |

**接続仕様:**
- ドライバ: psycopg（psycopg3系）
- メタデータ格納: JSONB（use_jsonb=True）
- ベクトル次元数: 384（埋め込みモデルに依存）

### 4.3 embeddings.py — テキスト埋め込み

LangChain の HuggingFaceEmbeddings ラッパーを使用。シングルトンパターンで遅延読み込み。

**関数:**

| 関数名 | 引数 | 戻り値 | 説明 |
|---|---|---|---|
| get_embeddings | なし | HuggingFaceEmbeddings | 埋め込みモデルインスタンスを返す |
| embed | texts: list[str] | list[list[float]] | テキストリストを384次元ベクトルリストに変換 |

**モデル仕様:**
- モデル: all-MiniLM-L6-v2
- 出力次元: 384
- 対応言語: 多言語（日本語含む）

### 4.4 llm.py — LLM推論

LangChain の LlamaCpp ラッパーを使用。GGUF 量子化モデルを CPU で推論する。シングルトンパターンで遅延読み込み。

**関数:**

| 関数名 | 引数 | 戻り値 | 説明 |
|---|---|---|---|
| get_llm | なし | LlamaCpp | LLMインスタンスを返す |
| generate | prompt: str | str | プロンプトを受け取り、LLMの生成テキストを返す |

**LLMパラメータ:**

| パラメータ | 値 | 説明 |
|---|---|---|
| model_path | ./models/llama-2-7b.Q4_K_M.gguf | GGUF形式のモデルファイル |
| n_ctx | 2048 | コンテキストウィンドウサイズ |
| max_tokens | 300 | 最大生成トークン数 |
| verbose | False | 推論ログ出力の抑制 |

### 4.5 reranker.py — Re-ranking

Cross-Encoder によるスコアリングで検索結果を再順位付けする。LangChain の ContextualCompressionRetriever パターンを採用。シングルトンパターンで遅延読み込み。

**関数:**

| 関数名 | 引数 | 戻り値 | 説明 |
|---|---|---|---|
| get_reranker | なし | CrossEncoderReranker | Re-rankerインスタンスを返す。top_n は RERANK_TOP_K に従う |
| get_compression_retriever | base_retriever | ContextualCompressionRetriever | ベースRetrieverをRe-rankerでラップしたRetrieverを返す |
| rerank | query, docs, top_k=3 | list[dict] | dict形式のドキュメントリストを受け取り、Re-ranking後のリストを返す。空リスト・None入力時は空リストを返す |

**Re-rankerモデル仕様:**
- モデル: ms-marco-MiniLM-L-6-v2（Cross-Encoder）
- 動作: CPU対応
- 入力: (query, document) ペアに対しスコアを算出

### 4.6 chunking.py — テキスト分割

外部ライブラリに依存しない独自実装のチャンク分割モジュール。

**関数:**

| 関数名 | 引数 | 戻り値 | 説明 |
|---|---|---|---|
| split_text | text, chunk_size=500, overlap=100 | list[str] | 固定サイズ分割。単語境界（空白）で分割位置を調整し、overlap分の重複を持たせる。None・空文字は空リストを返す |
| split_by_structure | text, chunk_size=None, overlap=100 | list[str] | 段落ベース分割。連続改行（\\n\\n）で段落に分割後、chunk_size指定時は長い段落をさらにsplit_textで分割する。chunk_size=None時は段落をそのまま返す |

**分割ルール:**
- split_text: テキスト長 <= chunk_size の場合は分割しない。分割位置は後方最寄りの空白を優先し、空白がない場合は前方に拡張。チャンク間の重複により文脈の断絶を防止
- split_by_structure: 空の段落（空白のみ）は除外。各段落の前後空白は除去

### 4.7 ingest.py — データ取り込みパイプライン

PDF と CSV を読み込み、チャンク分割・メタデータ付与のうえベクトルDBに一括格納する。

**関数:**

| 関数名 | 説明 |
|---|---|
| load_pdfs | data/pdf/ 配下の全PDFファイルをページ単位で読み込み、(テキスト, ソース) のタプルリストを返す |
| load_csvs | data/csv/ 配下の全CSVファイルを行単位で読み込み、各行を "カラム名:値" 形式のテキストに変換して返す |
| main | DB初期化 → PDF/CSV読み込み → チャンク分割 → メタデータ付与 → vectorstoreへ一括格納 |

**ソースID命名規則:**

| データ種別 | 形式 | 例 |
|---|---|---|
| PDF | ファイル名:p{ページ番号} | company_overview.pdf:p1 |
| CSV | ファイル名:r{行番号} | faq.csv:r1 |

※ページ番号・行番号はともに1始まり

**分割戦略の使い分け:**

| データ種別 | 分割方式 | 理由 |
|---|---|---|
| PDF | split_by_structure（段落ベース） | 文書構造（段落・セクション）を活かした分割 |
| CSV | RecursiveCharacterTextSplitter（LangChain） | 行単位テキストの固定サイズ分割 |

**メタデータ:**

各 Document オブジェクトに以下のメタデータを付与する。

| キー | 型 | 説明 |
|---|---|---|
| source | str | ソースID（上記命名規則に従う） |
| chunk_index | int | 同一ソース内でのチャンク連番（0始まり） |

### 4.8 ask.py — 質問応答 CLI

ユーザーの質問をコマンドライン引数で受け取り、LangGraph ワークフローを実行して回答とソースを出力する。

**関数:**

| 関数名 | 説明 |
|---|---|
| search | クエリを受け取り、ベクトル検索（SEARCH_K件）→ ContextualCompressionRetriever による Re-ranking を実行。結果を content/source キーの dict リストで返す |
| main | sys.argv[1] から質問を取得し、graph.py の LangGraph ワークフローを実行。回答とソース一覧を標準出力に表示 |

**出力フォーマット:**

```text
=== Answer ===
{LLMの生成回答}

=== Sources ===
- {ソースID1}
- {ソースID2}
```

### 4.9 graph.py — LangGraph ワークフロー定義

RAG パイプラインを LangGraph の StateGraph として定義する。シングルトンパターンで管理。

**関数:**

| 関数名 | 説明 |
|---|---|
| retrieve | vectorstore から SEARCH_K 件取得。state["query"] を入力とし documents を返す |
| rerank_node | Cross-Encoder でスコアリングし RERANK_TOP_K 件に絞り込む |
| generate_node | reranked_documents からコンテキストとソースを抽出し、日本語プロンプトを組み立てて LLM を呼び出す |
| build_rag_graph | StateGraph を構築・コンパイルして返す |
| get_graph | コンパイル済みグラフのシングルトンを返す |

**プロンプトテンプレート:**

```text
以下の情報を基に回答してください:

{コンテキストリスト}

質問:{ユーザーの質問}
回答:
```

### 4.10 metrics.py — 評価指標

RAG パイプラインの精度・品質を定量評価するための指標関数群。

**関数:**

| 関数名 | 引数 | 戻り値 | 説明 |
|---|---|---|---|
| retrieval_at_k | results, expected_source | bool | 検索結果リスト内に期待するソースIDが含まれるか判定。完全一致で比較 |
| faithfulness | answer, expected_keywords | float (0.0〜1.0) | 期待キーワードのうち回答に含まれるものの割合。キーワード空時は1.0 |
| exact_match | answer, expected_keywords | bool | 全キーワードが回答に含まれるか判定。キーワード空時はTrue |
| measure_latency | func | (result, float) | 関数の実行結果と経過秒数のタプルを返す |

### 4.11 evaluate.py — 評価パイプライン

評価用質問セットに対してパイプラインを実行し、精度レポートを出力する。

**関数:**

| 関数名 | 説明 |
|---|---|
| load_questions | data/eval_questions.json を読み込み、質問リストを返す |
| evaluate_single | 1問分の評価を実行。search → generate → 4指標（retrieval_hit, faithfulness, exact_match, latency）を算出 |
| run_evaluation | 全質問に対して evaluate_single を実行し、結果リストを返す |
| print_report | 設定情報と集計結果（Retrieval@k, Faithfulness, Exact Match, Latency）をフォーマット出力 |
| main | 全体のオーケストレーション |

**評価用質問セット（data/eval_questions.json）の構造:**

| フィールド | 型 | 説明 |
|---|---|---|
| query | str | 評価用の質問文 |
| expected_source | str | 正解のソースID（命名規則に従う） |
| expected_keywords | list[str] | 回答に含まれるべきキーワード |

現在13問を収録。CSV（FAQ・商品）および PDF（会社概要・技術ガイド）から出題。

**レポート出力フォーマット:**

```text
=== Evaluation Report ===

Chunk=500 overlap=100
Top-k=10
Re-rank=ON (top_k=3)

Retrieval@k: xx.x%
Faithfulness: xx.x%
Exact Match: xx.x%
Latency: xx.xs

Questions evaluated: 13
```

---

## 5. データベース仕様

### 5.1 構成

| 項目 | 値 |
|---|---|
| RDBMS | PostgreSQL 16 |
| 拡張 | pgvector |
| イメージ | pgvector/pgvector:pg16 |
| 接続ドライバ | psycopg（psycopg3系） |
| ORM/ラッパー | LangChain PGVector（langchain-postgres） |

### 5.2 データモデル

LangChain PGVector が管理するスキーマを使用。ユーザーが直接テーブルを定義する必要はない。

**主要カラム（LangChain PGVector 内部）:**

| カラム | 型 | 説明 |
|---|---|---|
| id | UUID | ドキュメント一意識別子 |
| collection_id | UUID | コレクションへの外部キー |
| document | TEXT | チャンクテキスト本文 |
| embedding | VECTOR(384) | 384次元埋め込みベクトル |
| cmetadata | JSONB | メタデータ（source, chunk_index） |

---

## 6. チャンク分割仕様

### 6.1 分割戦略

| 戦略 | 対象 | 分割単位 | 実装 |
|---|---|---|---|
| 段落ベース分割 | PDF | 連続改行（\\n\\n）で段落分割後、長い段落は固定サイズで再分割 | chunking.split_by_structure |
| 固定サイズ分割 | CSV | LangChain の RecursiveCharacterTextSplitter | langchain_text_splitters |

### 6.2 パラメータ

| パラメータ | デフォルト値 | 環境変数 | 説明 |
|---|---|---|---|
| chunk_size | 500文字 | CHUNK_SIZE | 1チャンクの最大文字数 |
| chunk_overlap | 100文字 | CHUNK_OVERLAP | チャンク間の重複文字数 |

### 6.3 分割時の制約

- 単語境界（空白）で分割位置を調整し、単語の途中で切断しない
- overlap が chunk_size 以上の場合でも無限ループにならない安全設計
- 空テキスト・None 入力時は空リストを返す
- 空白のみの段落は除外される

---

## 7. 環境変数一覧

| 変数名 | デフォルト値 | 型 | 説明 |
|---|---|---|---|
| DB_HOST | localhost | str | PostgreSQL ホスト名（Docker内では db） |
| DB_USER | rag | str | PostgreSQL ユーザー名 |
| DB_PASSWORD | rag | str | PostgreSQL パスワード |
| DB_NAME | rag | str | PostgreSQL データベース名 |
| CHUNK_SIZE | 500 | int | チャンク分割サイズ（文字数） |
| CHUNK_OVERLAP | 100 | int | チャンク間オーバーラップ（文字数） |
| SEARCH_K | 10 | int | ベクトル検索の取得件数 |
| RERANK_TOP_K | 3 | int | Re-ranking後の採用件数 |

※ 数値型の環境変数に非数値を設定した場合は ValueError が発生する。

---

## 8. Docker 構成

### 8.1 サービス構成

| サービス | イメージ | コンテナ名 | ポート | 説明 |
|---|---|---|---|---|
| db | pgvector/pgvector:pg16 | rag_db | 5432:5432 | PostgreSQL + pgvector |
| app | ローカルビルド | rag_app | なし | Python アプリケーション |

### 8.2 app コンテナ仕様

| 項目 | 値 |
|---|---|
| ベースイメージ | python:3.11-slim |
| 作業ディレクトリ | /app |
| ボリュームマウント | ホストのプロジェクトルート → /app |
| システム依存パッケージ | build-essential, git, curl, poppler-utils |
| DB接続 | 環境変数で DB_HOST=db を設定（Docker内部DNS） |

### 8.3 永続化

PostgreSQL データは名前付きボリューム pgdata で永続化。アプリケーションコンテナはボリュームマウントによりホストとコード共有。

---

## 9. 依存ライブラリ

### 9.1 コアライブラリ

| ライブラリ | バージョン要件 | 用途 |
|---|---|---|
| langchain | >= 0.3.0 | RAGフレームワーク基盤 |
| langchain-core | >= 0.3.0 | Document, Retriever 等の基本型 |
| langchain-community | >= 0.3.0 | LlamaCpp, HuggingFaceCrossEncoder |
| langchain-huggingface | >= 0.1.0 | HuggingFaceEmbeddings |
| langchain-postgres | >= 0.0.12 | PGVector ラッパー |
| langchain-text-splitters | >= 0.3.0 | RecursiveCharacterTextSplitter |
| langgraph | >= 0.2.0 | StateGraph によるワークフロー管理 |

### 9.2 ML / データ処理

| ライブラリ | 用途 |
|---|---|
| sentence-transformers | 埋め込みモデルのバックエンド |
| llama-cpp-python | GGUF形式 LLM の CPU推論 |
| pypdf | PDF テキスト抽出 |
| pandas | CSV 読み込み・行処理 |
| numpy | ベクトル演算 |

### 9.3 DB接続

| ライブラリ | 用途 |
|---|---|
| psycopg2-binary | PostgreSQL 接続（レガシー互換） |
| psycopg[binary] >= 3.1.0 | PostgreSQL 接続（langchain-postgres 用） |

---

## 10. CLI インターフェース（Makefile）

| コマンド | 説明 |
|---|---|
| make build | Docker イメージをビルド |
| make up | コンテナをバックグラウンド起動（PostgreSQL + app） |
| make down | コンテナを停止 |
| make shell | app コンテナに bash で入る |
| make test | 全テストを実行（pytest -v） |
| make lint | 全 app/ モジュールの構文チェック（py_compile） |
| make ingest | データ取り込みパイプラインを実行 |
| make ask Q="質問文" | RAG に質問して回答を取得 |
| make evaluate | 評価パイプラインを実行しレポート出力 |

全コマンドは python -m 形式で実行し、モジュールインポートの整合性を保証する。

---

## 11. シングルトンパターン

以下のモジュールは重量級リソース（モデルロード・DB接続）をシングルトンパターンで管理する。初回アクセス時に遅延読み込みし、以降は同一インスタンスを再利用する。

| モジュール | 管理対象 | グローバル変数 |
|---|---|---|
| db.py | PGVector インスタンス | _vectorstore |
| embeddings.py | HuggingFaceEmbeddings インスタンス | _embeddings |
| llm.py | LlamaCpp インスタンス | _llm |
| reranker.py | CrossEncoderReranker インスタンス | _reranker |
| graph.py | コンパイル済み StateGraph | _graph |

テスト時は各 fixture で対応するグローバル変数を None にリセットし、テスト間の干渉を防止する。

---

## 12. テスト方針

### 12.1 基本方針

- テストフレームワーク: pytest + pytest-mock
- モック戦略: sentence-transformers, llama-cpp-python, psycopg2 等の重量級依存はすべてモック化。テスト実行に DB・モデルファイルは不要
- テスト構成: pytest.ini で pythonpath = . を設定し、app.module 形式のインポートを使用

### 12.2 テスト分類

| 分類 | 目的 | 例 |
|---|---|---|
| 正常系 | 期待する入出力の検証 | search が content/source の dict リストを返す |
| 異常系 | エラー条件でのエラー種別の固定 | 存在しないファイルで FileNotFoundError |
| バリデーション | 境界値・不正入力への振る舞い固定 | None入力, 空リスト, sourceキー欠損 |
| シングルトン | 再利用・一度だけの初期化の保証 | get_llm の2回呼び出しで同一インスタンス |

### 12.3 共通フィクスチャ（conftest.py）

| フィクスチャ名 | 説明 |
|---|---|
| mock_db_connection | psycopg2 のコンテキストマネージャモック |
| fake_embeddings | 3×384 の NumPy 配列 |
| mock_llm_response | LLM レスポンスの dict モック |
| mock_vectorstore | LangChain vectorstore モック（as_retriever 対応） |
| mock_documents | source/chunk_index メタデータ付き Document リスト |

### 12.4 テスト規模

| テストファイル | 対象モジュール | テスト数 |
|---|---|---|
| test_config.py | config.py | 22 |
| test_db.py | db.py | 9 |
| test_embeddings.py | embeddings.py | 11 |
| test_llm.py | llm.py | 11 |
| test_chunking.py | chunking.py | 32 |
| test_reranker.py | reranker.py | 19 |
| test_ingest.py | ingest.py | 23 |
| test_ask.py | ask.py | 18 |
| test_graph.py | graph.py | 23 |
| test_metrics.py | metrics.py | 39 |
| test_evaluate.py | evaluate.py | 43 |
| **合計** | | **250** |

---

## 13. LLM モデル配置

| 項目 | 値 |
|---|---|
| モデル | Llama-2-7B |
| 量子化 | Q4_K_M（GGUF形式） |
| 配置先 | models/llama-2-7b.Q4_K_M.gguf |
| 推論方式 | CPU（llama-cpp-python経由） |
| 推奨環境 | CPU で Q4_K_M 量子化版を使用 |

---

## 14. データフロー詳細

### 14.1 取り込みフロー（make ingest）

```text
1. init_db()  →  PGVector接続確立
2. load_pdfs()  →  data/pdf/*.pdf をページ単位で読み込み
3. load_csvs()  →  data/csv/*.csv を行単位で読み込み
4. PDF: split_by_structure  →  段落ベースでチャンク分割
5. CSV: RecursiveCharacterTextSplitter  →  固定サイズでチャンク分割
6. 各チャンクに source, chunk_index メタデータを付与
7. vectorstore.add_documents()  →  一括格納（埋め込み生成はPGVector内部で実行）
```

### 14.2 質問応答フロー（make ask Q="..."）

```text
1. sys.argv[1] からクエリを取得
2. LangGraph ワークフローを実行:
   a. [retrieve]  pgvector コサイン距離検索で 10件取得
   b. [rerank]    Cross-Encoder で 3件に絞り込み
   c. [generate]  日本語プロンプト組立 → LLM生成
3. 回答テキストとソースIDを標準出力に表示
```

### 14.3 評価フロー（make evaluate）

```text
1. data/eval_questions.json から質問セットを読み込み
2. 各質問に対して:
   a. search()  →  ベクトル検索 + Re-ranking
   b. generate()  →  LLM回答生成
   c. retrieval_at_k  →  正解ソースが検索結果に含まれるか
   d. faithfulness  →  キーワード含有率
   e. exact_match  →  全キーワードの含有判定
   f. measure_latency  →  処理時間計測
3. 全質問の集計結果をレポート出力
```
