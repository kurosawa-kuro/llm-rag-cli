æ‰¿çŸ¥ã„ãŸã—ã¾ã—ãŸã€‚
**ã€Œ1æ—¥ã§å‹•ã CLIå‹ RAG + LLMï¼ˆCPUï¼‰ã€ã®æœ€å°æ§‹æˆ**ã‚’ã€Dockerè¾¼ã¿ã§æç¤ºã„ãŸã—ã¾ã™ã€‚
ï¼ˆAPIãªã—ï¼ãƒ•ã‚©ãƒ«ãƒ€å›ºå®šï¼Metaç³»ã¯å°†æ¥å·®ã—æ›¿ãˆå‰æã€‚CPUã§ã¯ `llama.cpp` ã‚’ä½¿ç”¨ï¼‰

---

# ğŸ“¦ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹æˆ

```
rag-cli/
â”œâ”€ data/
â”‚  â”œâ”€ pdf/
â”‚  â””â”€ csv/
â”œâ”€ app/
â”‚  â”œâ”€ ingest.py
â”‚  â”œâ”€ ask.py
â”‚  â”œâ”€ db.py
â”‚  â”œâ”€ embeddings.py
â”‚  â”œâ”€ llm.py
â”‚  â””â”€ config.py
â”œâ”€ requirements.txt
â”œâ”€ docker-compose.yml
â”œâ”€ Dockerfile
â””â”€ README.md
```

---

# ğŸ³ docker-compose.yml

```yaml
version: "3.9"

services:
  db:
    image: pgvector/pgvector:pg16
    container_name: rag_db
    environment:
      POSTGRES_USER: rag
      POSTGRES_PASSWORD: rag
      POSTGRES_DB: rag
    ports:
      - "5432:5432"
    volumes:
      - pgdata:/var/lib/postgresql/data

  app:
    build: .
    container_name: rag_app
    depends_on:
      - db
    volumes:
      - .:/app
    environment:
      DB_HOST: db
      DB_USER: rag
      DB_PASSWORD: rag
      DB_NAME: rag
    tty: true

volumes:
  pgdata:
```

---

# ğŸ³ Dockerfile

```dockerfile
FROM python:3.11-slim

WORKDIR /app

RUN apt-get update && apt-get install -y \
    build-essential \
    git \
    curl \
    poppler-utils \
    && rm -rf /var/lib/apt/lists/*

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

CMD ["bash"]
```

---

# ğŸ“¦ requirements.txt

```txt
psycopg2-binary
sentence-transformers
pypdf
pandas
numpy
tqdm
llama-cpp-python
```

---

# âš™ config.py

```python
import os

DB_CONFIG = {
    "host": os.getenv("DB_HOST", "localhost"),
    "user": os.getenv("DB_USER", "rag"),
    "password": os.getenv("DB_PASSWORD", "rag"),
    "dbname": os.getenv("DB_NAME", "rag"),
}

EMBED_MODEL = "sentence-transformers/all-MiniLM-L6-v2"

LLM_MODEL_PATH = "./models/llama-2-7b.Q4_K_M.gguf"
```

---

# ğŸ˜ db.py

```python
import psycopg2
from config import DB_CONFIG

def get_conn():
    return psycopg2.connect(**DB_CONFIG)

def init_db():
    with get_conn() as conn:
        with conn.cursor() as cur:
            cur.execute("CREATE EXTENSION IF NOT EXISTS vector;")
            cur.execute("""
            CREATE TABLE IF NOT EXISTS documents (
                id SERIAL PRIMARY KEY,
                content TEXT,
                embedding VECTOR(384)
            );
            """)
        conn.commit()
```

---

# ğŸ§  embeddings.py

```python
from sentence_transformers import SentenceTransformer
from config import EMBED_MODEL

model = SentenceTransformer(EMBED_MODEL)

def embed(texts):
    return model.encode(texts)
```

---

# ğŸ“¥ ingest.py

```python
import os
from pypdf import PdfReader
import pandas as pd
from db import init_db, get_conn
from embeddings import embed
from tqdm import tqdm

DATA_DIR = "data"

def load_pdfs():
    texts = []
    for file in os.listdir(f"{DATA_DIR}/pdf"):
        if file.endswith(".pdf"):
            reader = PdfReader(f"{DATA_DIR}/pdf/{file}")
            for page in reader.pages:
                texts.append(page.extract_text())
    return texts

def load_csvs():
    texts = []
    for file in os.listdir(f"{DATA_DIR}/csv"):
        if file.endswith(".csv"):
            df = pd.read_csv(f"{DATA_DIR}/csv/{file}")
            for _, row in df.iterrows():
                texts.append(" ".join([f"{k}:{v}" for k, v in row.items()]))
    return texts

def main():
    init_db()
    texts = load_pdfs() + load_csvs()
    vectors = embed(texts)

    with get_conn() as conn:
        with conn.cursor() as cur:
            for text, vec in tqdm(zip(texts, vectors)):
                cur.execute(
                    "INSERT INTO documents (content, embedding) VALUES (%s, %s)",
                    (text, vec.tolist())
                )
        conn.commit()

if __name__ == "__main__":
    main()
```

---

# ğŸ¤– llm.py

```python
from llama_cpp import Llama
from config import LLM_MODEL_PATH

llm = Llama(model_path=LLM_MODEL_PATH, n_ctx=2048)

def generate(prompt):
    output = llm(prompt, max_tokens=300)
    return output["choices"][0]["text"]
```

---

# â“ ask.py

```python
import sys
from db import get_conn
from embeddings import embed
from llm import generate

def search(query, k=3):
    vec = embed([query])[0].tolist()
    with get_conn() as conn:
        with conn.cursor() as cur:
            cur.execute(
                "SELECT content FROM documents ORDER BY embedding <-> %s LIMIT %s;",
                (vec, k)
            )
            return [row[0] for row in cur.fetchall()]

def main():
    query = sys.argv[1]
    contexts = search(query)
    prompt = f"ä»¥ä¸‹ã®æƒ…å ±ã‚’åŸºã«å›ç­”ã—ã¦ãã ã•ã„:\n\n{contexts}\n\nè³ªå•:{query}\nå›ç­”:"
    answer = generate(prompt)
    print("\n=== Answer ===\n")
    print(answer)

if __name__ == "__main__":
    main()
```

---

# ğŸš€ å®Ÿè¡Œæ‰‹é †

```bash
docker compose up -d
docker compose exec app bash
python app/ingest.py
python app/ask.py "åˆ¶åº¦ã®ç›®çš„ã¯ï¼Ÿ"
```

---

# ğŸ§  LLMãƒ¢ãƒ‡ãƒ«å–å¾—ï¼ˆä¾‹ï¼‰

Metaç³»ãƒ¢ãƒ‡ãƒ«ï¼ˆä¾‹ï¼šLlama 2 7Bï¼‰ã‚’
GGUFé‡å­åŒ–ç‰ˆã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ï¼š

```
models/llama-2-7b.Q4_K_M.gguf
```

ã«é…ç½®ã€‚

ï¼ˆCPUãªã‚‰Q4_K_Mæ¨å¥¨ï¼‰

---

# ğŸ¯ ã“ã‚Œã§é”æˆã§ãã‚‹ã“ã¨

* CLIå‹RAGå®Œæˆ
* PDF + CSVå¯¾å¿œ
* pgvectoræ¤œç´¢
* CPUæ¨è«–
* AWSç§»è¡Œå®¹æ˜“ï¼ˆDBã¨LLMå·®ã—æ›¿ãˆã®ã¿ï¼‰

---

ã‚‚ã—ã‚ˆã‚ã—ã‘ã‚Œã°æ¬¡ã¯ï¼š

* ğŸ”¥ ã•ã‚‰ã«è»½é‡åŒ–ç‰ˆï¼ˆTinyLlamaç”¨ï¼‰
* â˜ AWS GPUç§»è¡Œå·®åˆ†ãƒ‘ãƒƒãƒ
* ğŸ“Š ç²¾åº¦è©•ä¾¡ã‚¹ã‚¯ãƒªãƒ—ãƒˆè¿½åŠ 
* ğŸ³ Makefileä»˜ãé«˜é€Ÿé–‹ç™ºç‰ˆ

ã©ã“ã¾ã§æœ¬æ°—ã§ä»•ä¸Šã’ã¾ã™ã‹ï¼Ÿ
